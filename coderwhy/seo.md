

### coderwhy前端八股文（一）

SEO相关的题目在面试被问的较多，一方面是SEO对于Web站点很重要，另一方面很多同学的简历中都会写到SEO相关的表述。

另外SEO是一个相对宽泛的话题，所以回答的好坏差别会很大。

---

#### 01-什么是SEO（搜索引擎优化）？为什么SEO对于一个网站至关重要？

SEO是搜索引擎优化，它的英文全称是**Search Engine Optimization**。

- 因为现在很多我们开发的网站，类似于门户网站或者功能网站（比如我之前在企业里面开发的弘源旅途、星客SC、网易云音乐），一方面我们需要通过营销宣传来提升我们产品的知名度，另一方面靠自然搜索结果获取流量也是一个非常重要的过程。
- 所以我们需要了解一定的搜索引擎自然排名机制的基础上，对我们的网站进行内部和外部的调整优化，让用户在使用关键词搜索时找到我们的网站可以尽量高的提升自然排名，获取更多的流量，从而达到我们预期的销量以及品牌的知名度。

**关键点：**
- **关键一**：回答出问题本身，比如说问到你SEO优化是什么了，你就要回答出来。
- **关键二**：结合真实的项目或者案例来回答，越回答对应的项目，越真实，而不是你背诵的八股文。
- **关键三**：可以做一定的引导，会引导到你做的项目上，我们提前针对项目准备的很多问题就能派上用场了。

---

#### 02-SEO有哪些关键点？你在日常开发中，都采取了哪些措施来进行SEO呢？

从重要到次要依次来说明，如果中间面试官开始根据你的回答问其他问题，那么就可以暂时停下来回答其他问题。

当然目前在国内针对百度有一个很直接的SEO方式就是给钱，其实有可以通过不花钱的方式提升网站关键词排名的方案。

---

#### 方式一：SSR服务器端渲染

因为我之前企业的项目，包括现在很多现在新的项目，都是基于现代的框架，比如Vue、React来开发的，大部分页面元素是由客户端JavaScript动态生成的。很多的搜索引擎，在爬虫时只能抓取静态的HTML源代码，而不会执行JavaScript，因此动态生成的内容无法被爬虫索引。另外很多的搜索情景下不会等待一部数据加载完成后再进行抓取，也会导致我们网站的很多关键词信息不能被完整的收录。

为了确保网站的SEO优化，我们之前的项目需要SEO优化的都采用了SSR技术。SSR能够在服务器上执行JavaScript并渲染出完整的HTML页面，然后将其发送到客户端。这样，爬虫在抓取网站时就能获取到完整的页面内容，从而提升SEO效果。

如果是开发初期就计算进行SEO优化的话，我们一般会直接选择一些比较成熟的SSR框架，比如对于Vue来说选择Nuxt.js，对于React来说选择Next.js。

---

#### 方式二：准确的TDK描述

TDK就是我们常说的（面试）title、description、keywords。

- **Title（标题）**：也就是网站显示的标题，不仅仅用户会看到，搜索引擎通常会首先检索和收录title信息，所以title至关重要。title一般不需要过长，多个关键词之间使用“|”或者“-”分割，会被搜索引擎提取和收录。
- **Description（描述）**：这是对网页内容的简短描述，通常在搜索引擎结果页中标题下方显示。描述应概述页面内容，包含相关关键词，并吸引用户点击。
- **Keywords（关键词）**：这是网页内容中重要的词汇，反映了页面的主题和内容。每个关键词都要有对应的内容匹配。虽然现代搜索引擎（如Google）对关键词标签的重视程度已经降低，但在某些情况下，合理使用关键词仍然有助于SEO。

---

#### 方式三：语义化的HTML元素、图片alt、h1、h2的合理使用

语义化的HTML代码和符合W3C规范是SEO的关键要素之一。

- **语义化**是指使用具有明确含义的HTML元素，搜索引擎在爬取网站时，也会更容易理解网站的内容以便进行收录，从侧面也能印证我们的网站更加的规范。而且这不仅有助于搜索引擎理解网页内容，还能提高网页的可读性和可维护性。
  - 包括Header、Nav、Aside、Article、Footer元素，这些都能帮助爬虫更好的获取页面内容、理解网页。

**图片要求必须加alt规范**
- 我们要求每个前端在使用图片时，必须加上和图片相关的alt，一方面是图片无法显示时用户可以看到提示，另一方面也有利于SEO优化。

**重要的标签h1/h2/h3等的使用**
- H1、H2、H3等HTML标题标签在SEO中起着非常重要的作用。这些标签有助于搜索引擎理解网页内容的结构和层次，从而更准确地索引和评估页面的相关性。

---

#### 方式四：编写合理的robots.txt文件

`robots.txt`是一个存放在网站根目录中的文本文件，其主要作用是告诉搜索引擎爬虫哪些部分的网站可以被抓取（爬取）以及哪些部分不应该被抓取。

**为什么需要使用`robots.txt`**
- 通过指示搜索引擎忽略不重要的文件或目录，可以让搜索引擎专注于重要内容的抓取和索引。
- 当然也可以避免一些敏感或私有内容被无意中索引。

所以如果网站不编写robots.txt，可能会降低网站的SEO效率，因为搜索引擎花费更多时间和资源在不重要的页面上。

**举几个例子：**
- 知乎
- 爱彼迎

---

#### 方式五：HTTPS

自2014年以来，Google已将HTTPS作为其搜索排名的信号之一。这意味着，使用HTTPS的网站在搜索结果中可能会获得比非HTTPS网站更好的排名。

而且HTTPS也有利于用户的安全，在用户使用网站时也会增加信任度。

---

#### 方式六：内部链接和外部链接

- **内部链接**是指从一个页面到同一网站内另一个页面的链接。它可以提高网站导航、增强网站的权重、提升网站的索引。
- **外部链接**是指从一个网站指向另一个网站的链接。在网页中放合适的外部链接，也有利于提升网站的权重指数，容易被搜索引擎收录。

---

#### 其他方式

当然还有一些其他的细节，比如sitemap文件、网站导航、响应式的处理，都在某种程度上能提高网站的权重。另外有一些企业还会专门请一些SEO的专员来进行SEO优化的操作，每个企业情况不太一样。

